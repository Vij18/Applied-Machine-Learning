{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tg9fRSjFtsRu"
   },
   "source": [
    "# Assignment #3\n",
    "## P556: Applied Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jjsZpx5C9eBH"
   },
   "source": [
    "More often than not, we will use a deep learning library (Tensorflow, Pytorch, or the wrapper known as Keras) to implement our models. However, the abstraction afforded by those libraries can make it hard to troubleshoot issues if we don't understand what is going on under the hood. In this assignment you will implement a fully-connected and a convolutional neural network from scratch. To simplify the implementation, we are asking you to implement static architectures, but you are free to support variable number of layers/neurons/activations/optimizers/etc. We recommend that you make use of private methods so you can easily troubleshoot small parts of your model as you develop them, instead of trying to figure out which parts are not working correctly after implementing everything. Also, keep in mind that there is code from your fully-connected neural network that can be re-used on the CNN. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2NzW9M-btzqO"
   },
   "source": [
    "Problem #1.1 (40 points): Implement a fully-connected neural network from scratch. The neural network will have the following architecture:\n",
    "\n",
    "- Input layer\n",
    "- Dense hidden layer with 512 neurons, using relu as the activation function\n",
    "- Dropout with a value of 0.2\n",
    "- Dense hidden layer with 512 neurons, using relu as the activation function\n",
    "- Dropout with a value of 0.2\n",
    "- Output layer, using softmax as the activation function\n",
    "\n",
    "The model will use categorical crossentropy as its loss function. \n",
    "We will optimize the gradient descent using RMSProp, with a learning rate of 0.001 and a rho value of 0.9.\n",
    "We will evaluate the model using accuracy.\n",
    "\n",
    "Why this architecture? We are trying to reproduce from scratch the following [example from the Keras documentation](https://keras.io/examples/mnist_mlp/). This means that you can compare your results by running the Keras code provided above to see if you are on the right track."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8rPUmRqBtpS2"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "#RMS prop var\n",
    "\n",
    "\n",
    "class NeuralLayer_weights():\n",
    "    def __init__(self,Nodes_present_layer,Nodes_next_layer):\n",
    "        self.Present_Nodes=Nodes_present_layer\n",
    "        self.Next_Nodes=Nodes_next_layer\n",
    "        self.Weights_vector=np.random.rand(Nodes_present_layer,Nodes_next_layer)\n",
    "        self.grad_sq=np.zeros((Nodes_present_layer,Nodes_next_layer))\n",
    "        #self.input_nodes=0\n",
    "        #self.output_nodes=0\n",
    "    \n",
    "    def getWeights(self):\n",
    "        return self.Weights_vector\n",
    "    \n",
    "    #updating the weights after back propagation\n",
    "    def setWeights(self,updated_weights):\n",
    "        self.Weights_vector=updated_weights\n",
    "        pass\n",
    "\n",
    "    def getRMSPROP(self):\n",
    "        return self.grad_sq\n",
    "    \n",
    "    def updateRMSprop(self,new_weights):\n",
    "        self.grad_sq=new_weights\n",
    "        pass\n",
    "        \n",
    "    def input_node_initial(self,input_nodes):\n",
    "        self.input_node_initial=input_nodes\n",
    "    \n",
    "    def output_node_initial(self, output_nodes):\n",
    "        self.output_node_inital= output_nodes\n",
    "        \n",
    "    def delta_value(self,delta):\n",
    "        self.delta_value=delta    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "class NeuralNetwork():\n",
    "    def __init__(self,epochs, learning_rate,Num_Layer, NodesPerLayer, Dropout_val,rho):\n",
    "        self.epochs=epochs\n",
    "        self.learning_rate=learning_rate\n",
    "        self.Num_Layer=Num_Layer\n",
    "        self.NodesPerLayer=NodesPerLayer\n",
    "        self.all_layers_weights=0\n",
    "        self.Dropout_val=Dropout_val\n",
    "        self.rho=rho\n",
    "        \n",
    "    #relu activation ftn--> takes vect as input and returns tranformed vector\n",
    "    def relu_activation(self,input_units):\n",
    "        # relu activation function is 0 for all values less than 0\n",
    "        cost_ftn=[0 if units<0 else units for units in input_units]\n",
    "        return cost_ftn \n",
    "    \n",
    "    #return the softmax output\n",
    "    def softmax_activation_ftn(self,z):\n",
    "        \"\"\"# softmax = e^(y)/sum(e^(Y))\n",
    "        \n",
    "        output_exp=[np.exp(y) for y in output]\n",
    "        print(output)\n",
    "        sum_output_exp=sum(output_exp)\n",
    "        softmax_out=[out/sum_output_exp for out in output_exp]\n",
    "        return softmax_out\"\"\"\n",
    "        z_norm=np.exp(z-np.max(z,axis=0,keepdims=True))\n",
    "        return(np.divide(z_norm,np.sum(z_norm,axis=0,keepdims=True)))\n",
    "    \n",
    "    #initialization of weights for all the layers-->x_train(vec) input\n",
    "    def Layers_weight_initialization(self,x_train):\n",
    "        input_nodes=x_train.shape[1]\n",
    "        Nodes_Input=[input_nodes]+self.NodesPerLayer\n",
    "        Nodes_output=self.NodesPerLayer+[self.NodesPerLayer[-1]]\n",
    "        layers_weights=[]\n",
    "        for (i,j) in zip(Nodes_Input,Nodes_output):\n",
    "            layers_weights.append(NeuralLayer_weights(i,j))\n",
    "        self.all_layers_weights=layers_weights\n",
    "        pass\n",
    "\n",
    "    #calculates the cross entropy on trained data \n",
    "    def cross_entropy(self,y_pred,y_act):\n",
    "      prob_y=[i/sum(y_pred) for i in y_pred]\n",
    "      err=0\n",
    "      for i in range(len(prob_y)):\n",
    "          if y_act[i]==1:\n",
    "              err=err+(-np.log(prob_y[i]))\n",
    "          else:\n",
    "              err=err+(-np.log(1-prob_y[i]))\n",
    "      return err\n",
    "\n",
    "    #fits the training data\n",
    "    def fit(self,x_train,y_train):\n",
    "        #x_train_bias=np.hstack((np.ones(len(x_train),1)),x_train)\n",
    "        self.Layers_weight_initialization(x_train)\n",
    "        accuracy=[]\n",
    "        for epoch in range(0,epochs):\n",
    "            for (x,y) in zip(x_train,y_train):\n",
    "                (activation_val,z)=self.feed_forward(x)\n",
    "                delta=self.backprop(y,activation_val,z)\n",
    "                self.updateweights(delta,z,activation_val)\n",
    "                accuracy.append(self.cross_entropy(activation_val[self.Num_Layer-1],y))\n",
    "            print(\"Cross Entropy error on epoch = \"+str(epoch+1)+\" is \"+str(np.mean(accuracy)))\n",
    "\n",
    "    #checks whether the prediction belongs to correct class\n",
    "    def evaluate(self,y_actual,y_calcualte):\n",
    "        #return -np.sum(np.log(y_calcualte) * y_actual, axis=1)\n",
    "        return y_actual==y_calcualte\n",
    "           \n",
    "    def feed_forward(self,x):\n",
    "        activation_val=[]\n",
    "        z=[]\n",
    "        for (layer_index,layer) in enumerate(self.all_layers_weights):\n",
    "            if layer_index==0:\n",
    "                activation_val.append(x)\n",
    "                #z.append(x)\n",
    "                weight=layer.getWeights()\n",
    "            else:\n",
    "                z.append(np.dot(np.transpose(activation_val[layer_index-1]),weight)+1)\n",
    "                weight=layer.getWeights()\n",
    "                activation=self.relu_activation(z[layer_index-1])\n",
    "                if layer_index!=self.Num_Layer-1:\n",
    "                    activation_val.append(activation)\n",
    "                else:\n",
    "                    #activation_val.append(self.softmax_activation_ftn(activation))\n",
    "                    activation_val.append(activation)\n",
    "        return (activation_val,z)\n",
    "    \n",
    "    def backprop(self,y_train,activation_val,z):\n",
    "        delta=[]\n",
    "        for (layer_index,layer) in enumerate(self.all_layers_weights[::-1]):\n",
    "            if layer_index==0:\n",
    "                flag=activation_val[self.Num_Layer-1]-y_train\n",
    "                delta.append(flag)\n",
    "            else:\n",
    "                flag=np.dot(layer.getWeights(),delta[layer_index-1])\n",
    "                g_prime_val=self.relu_prime(z[self.Num_Layer-2-layer_index])\n",
    "                delta.append(flag*g_prime_val)  \n",
    "        delta=np.array(delta[::-1])\n",
    "        return delta[1:]\n",
    "    \n",
    "    def relu_prime(self,x):\n",
    "        return np.max(np.sign(x),0)\n",
    "        \n",
    "    def updateweights(self,delta,z,activation_val):\n",
    "        for currentLayer,index in zip(self.all_layers_weights[:-1],range(self.Num_Layer-1)):\n",
    "            curr_weights=currentLayer.getWeights()\n",
    "            if index==0:\n",
    "                delta_v=delta[index].reshape(-1,1)\n",
    "                act_val=activation_val[index].reshape(-1,1)\n",
    "                grad=np.dot(act_val,delta_v.T)*10e-18\n",
    "            else:\n",
    "                delta_v=delta[index].reshape(-1,1)\n",
    "                act_val=z[index-1].reshape(-1,1)\n",
    "                grad=np.dot(act_val,delta_v.T)*10e-18\n",
    "            rms_var=(self.rho)*currentLayer.getRMSPROP() + (1-self.rho)*((grad)**2)\n",
    "            currentLayer.updateRMSprop(rms_var)\n",
    "            #print(rms_var)\n",
    "            curr_weights=curr_weights-((self.learning_rate)*np.sqrt(currentLayer.getRMSPROP())*grad)\n",
    "            currentLayer.setWeights(curr_weights)\n",
    "\n",
    "    def predict(self,xtest,ytest):\n",
    "        accuracy=[]\n",
    "        for (x,y) in zip(xtest,ytest):\n",
    "                (activation_val,z)=self.feed_forward(x)\n",
    "                accuracy.append(self.evaluate(self.softmax_activation_ftn(activation_val[self.Num_Layer-1]),y))\n",
    "        return str(np.mean(accuracy)*100)\n",
    "                                  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DH3bgJyPuE2O"
   },
   "source": [
    "Problem #1.2 (10 points): Train your fully-connected neural network on the Fashion-MNIST dataset using 5-fold cross validation. Report accuracy on the folds, as well as on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 676
    },
    "colab_type": "code",
    "id": "XsN4sUoUugl8",
    "outputId": "fddb12b1-44e1-432e-e290-1ca953930a81"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n",
      "Cross Entropy error on epoch = 1 is 3.2547626147868054\n",
      "Cross Entropy error on epoch = 2 is 3.254762614786805\n",
      "Cross Entropy error on epoch = 3 is 3.2547626147868045\n",
      "Cross Entropy error on epoch = 4 is 3.2547626147868045\n",
      "Cross Entropy error on epoch = 5 is 3.254762614786805\n",
      "The Accuracy on the 10000 testing data-set is 81.452\n",
      "Cross Entropy error on epoch = 1 is 3.251656173004386\n",
      "Cross Entropy error on epoch = 2 is 3.251656173004386\n",
      "Cross Entropy error on epoch = 3 is 3.251656173004386\n",
      "Cross Entropy error on epoch = 4 is 3.251656173004386\n",
      "Cross Entropy error on epoch = 5 is 3.2516561730043865\n",
      "The Accuracy on the k-fold testing data-set is 66.25583333333334\n",
      "Cross Entropy error on epoch = 1 is 3.255098772122755\n",
      "Cross Entropy error on epoch = 2 is 3.2550987721227544\n",
      "Cross Entropy error on epoch = 3 is 3.255098772122755\n",
      "Cross Entropy error on epoch = 4 is 3.2550987721227544\n",
      "Cross Entropy error on epoch = 5 is 3.255098772122754\n",
      "The Accuracy on the k-fold testing data-set is 81.98083333333334\n",
      "Cross Entropy error on epoch = 1 is 3.25485608235838\n",
      "Cross Entropy error on epoch = 2 is 3.25485608235838\n",
      "Cross Entropy error on epoch = 3 is 3.25485608235838\n",
      "Cross Entropy error on epoch = 4 is 3.25485608235838\n",
      "Cross Entropy error on epoch = 5 is 3.2548560823583808\n",
      "The Accuracy on the k-fold testing data-set is 81.925\n",
      "Cross Entropy error on epoch = 1 is 3.2526771805258146\n",
      "Cross Entropy error on epoch = 2 is 3.2526771805258132\n",
      "Cross Entropy error on epoch = 3 is 3.2526771805258137\n",
      "Cross Entropy error on epoch = 4 is 3.252677180525814\n",
      "Cross Entropy error on epoch = 5 is 3.2526771805258146\n",
      "The Accuracy on the k-fold testing data-set is 78.33833333333334\n",
      "Cross Entropy error on epoch = 1 is 3.2565170470214775\n",
      "Cross Entropy error on epoch = 2 is 3.2565170470214775\n",
      "Cross Entropy error on epoch = 3 is 3.2565170470214784\n",
      "Cross Entropy error on epoch = 4 is 3.256517047021478\n",
      "Cross Entropy error on epoch = 5 is 3.2565170470214784\n",
      "The Accuracy on the k-fold testing data-set is 79.9275\n"
     ]
    }
   ],
   "source": [
    "# To simplify the usage of our dataset, we will be importing it from the Keras \n",
    "# library. Keras can be installed using pip: python -m pip install keras\n",
    "\n",
    "# Original source for the dataset:\n",
    "# https://github.com/zalandoresearch/fashion-mnist\n",
    "\n",
    "# Reference to the Fashion-MNIST's Keras function: \n",
    "# https://keras.io/datasets/#fashion-mnist-database-of-fashion-articles\n",
    "\n",
    "from keras.datasets import fashion_mnist\n",
    "import keras\n",
    "\n",
    "# the data, split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "x_train = x_train.reshape(60000, 784)\n",
    "x_test = x_test.reshape(10000, 784)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "num_classes = 10\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "## creating Neural Network\n",
    "\n",
    "Number_layers=4\n",
    "Nodes_layer=[512,512,num_classes]\n",
    "learning_rate=0.001\n",
    "epochs=5\n",
    "Dropout_val=0.2\n",
    "rho=0.9\n",
    "\n",
    "myNet=NeuralNetwork(epochs, learning_rate,Number_layers,Nodes_layer,Dropout_val,rho)\n",
    "myNet.fit(x_train,y_train)\n",
    "accuracy=myNet.predict(x_test,y_test)\n",
    "print(\"The Accuracy on the 10000 testing data-set is \"+str(accuracy))\n",
    "\n",
    "\n",
    "# k fold cross validation\n",
    "from sklearn.model_selection import KFold\n",
    "cv = KFold(n_splits=5, shuffle=False)\n",
    "for train_index, test_index in cv.split(x_train):\n",
    "    Number_layers=4\n",
    "    Nodes_layer=[50,50,num_classes]\n",
    "    learning_rate=0.001\n",
    "    epochs=5\n",
    "    Dropout_val=0.2\n",
    "    rho=0.9\n",
    "    myNet_k=NeuralNetwork(epochs, learning_rate,Number_layers,Nodes_layer,Dropout_val,rho)\n",
    "    x_train_k=x_train[train_index]\n",
    "    y_train_k=y_train[train_index]\n",
    "    myNet_k.fit(x_train_k,y_train_k)\n",
    "    x_test_k=x_train[test_index]\n",
    "    y_test_k=y_train[test_index]\n",
    "    accuracy=myNet_k.predict(x_test_k,y_test_k)\n",
    "    print(\"The Accuracy on the k-fold testing data-set is \"+str(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U8pn5rPQiTwz"
   },
   "source": [
    "#### The first 5 lines in the output gives the information about the normal test and train implementation i.e. Trained with 60000 images and tested on 10000 images\n",
    "\n",
    "#### The next set of output represent 5 different iteration of k=5 fold implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ldXmpreT-N0x"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "A3_P556_F19 - Copy.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
